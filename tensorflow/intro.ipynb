{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensortFlow\n",
    "\n",
    "**OS X or Linux**\n",
    "Run the following commands to setup your environment:\n",
    "```\n",
    "conda create -n tensorflow python=3.5\n",
    "source activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "**Windows**\n",
    "And installing on Windows. In your console or Anaconda shell,\n",
    "```\n",
    "conda create -n tensorflow python=3.5\n",
    "activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "![alt text](session.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "In the last section, you passed a tensor into a session and it returned the result. What if you want to use a non-constant? This is where **tf.placeholder()** and feed_dict come into place. In this section, you'll go over the basics of feeding data into TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    x = tf.placeholder(tf.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed the x tensor 123\n",
    "        output = sess.run(x, feed_dict={x: 123})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print(run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "# x = 10\n",
    "# y = 2\n",
    "# z = x/y - 1\n",
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x, y), tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to initalize variables\n",
    "The **tf.Variable** class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the **tf.global_variables_initializer()** function to initialize the state of all the Variable tensors.\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.\n",
    "\n",
    "```python\n",
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "with tf.Session() as session:\n",
    "    #Initialize session variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "```\n",
    "\n",
    "The **tf.truncated_normal()** function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier Quiz\n",
    "You'll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow. The above is a small sample of the data you'll be training on. Notice how some of the 1s are written with a serif at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model.\n",
    "\n",
    "![alt text](mnist-012.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"quiz_solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Loss: 4.666358470916748\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"sandbox_solution.py\" tab\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    #mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "    mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Softmax\n",
    "The softmax function squashes it's inputs, typically called logits or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1. This means the output of the softmax function is equivalent to a categorical probability distribution. It's the perfect function to use as the output activation for a network predicting multiple classes.\n",
    "\n",
    "![alt text](softmax-input-output.png)\n",
    "\n",
    "```python\n",
    "x = tf.nn.softmax([2.0, 1.0, 0.2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65900117  0.24243298  0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # TODO: Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)   \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "\n",
    "print(run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding With Scikit-Learn\n",
    "Transforming your labels into one-hot encoded vectors is pretty simple with scikit-learn using LabelBinarizer. Check it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Example labels\n",
    "labels = np.array([1,5,3,2,1,4,2,1,3])\n",
    "\n",
    "# Create the encoder\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Here the encoder finds the classes and assigns one-hot vectors \n",
    "lb.fit(labels)\n",
    "\n",
    "# And finally, transform the labels into one-hot encoded vectors\n",
    "lb.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3025850929940455"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# natural logarithm\n",
    "math.log(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy in TensorFlow\n",
    "As with the softmax function, TensorFlow has a function to do the cross entropy calculations for us.\n",
    "![alt text](cross-entropy-diagram.png)\n",
    "\n",
    "#### Reduce Sum\n",
    "\n",
    "**`x = tf.reduce_sum([1, 2, 3, 4, 5])  #15`**\n",
    "The tf.reduce_sum() function takes an array of numbers and sums them together.\n",
    "\n",
    "#### Natural Log\n",
    "\n",
    "**`x = tf.log(100)  # 4.60517`**\n",
    "This function does exactly what you would expect it to do. tf.log() takes the natural log of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "# Print the cross entropy using softmax_data and one_hot_encod_label.\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    #session.run(tf.global_variables_initializer())\n",
    "    output = session.run(cross_entropy, feed_dict={softmax:softmax_data, one_hot:one_hot_data})\n",
    "    print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95367431640625\n"
     ]
    }
   ],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPUTE MEMORY COST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "train_features(55000, 784): 172480000\n",
      "train_labels(55000, 10): 2200000\n",
      "weights(784, 10): 31360\n",
      "bias(10,): 40\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "## COMPUTE MEMORY COST\n",
    "print(\"train_features{}: {}\".format(train_features.shape, train_features.shape[0]*train_features.shape[1]*4))\n",
    "print(\"train_labels{}: {}\".format(train_labels.shape, train_labels.shape[0]*train_labels.shape[1]*4))\n",
    "print(\"weights{}: {}\".format(weights.shape, weights.shape[0]*weights.shape[1]*4))\n",
    "print(\"bias{}: {}\".format(bias.shape, bias.shape[0]*4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Mini-batching\n",
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes.\n",
    "\n",
    "Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes].\n",
    "\n",
    "```python\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "```\n",
    "\n",
    "The `None` dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batch: 391, last batch: 80\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "dataset_samples = 50000\n",
    "batch_size = 128\n",
    "num_batches = math.ceil(dataset_samples / batch_size)\n",
    "last_batch_size = dataset_samples % batch_size\n",
    "print(\"num batch: {}, last batch: {}\".format(num_batches, last_batch_size))\n",
    "if last_batch_size > 0: \n",
    "    assert(dataset_samples ==  (num_batches - 1) * batch_size + last_batch_size )\n",
    "else:\n",
    "    assert(dataset_samples ==  num_batches * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating batches\n",
    "\n",
    "Implement the batches function to batch features and labels. The function should return each batch with a maximum size of batch_size. To help you with the quiz, look at the following example output of a working batches function.\n",
    "\n",
    "```python\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "```\n",
    "example_batches = batches(3, example_features, example_labels)\n",
    "The example_batches variable would be the following:\n",
    "\n",
    "```python\n",
    "[\n",
    "    # 2 batches:\n",
    "    #   First is a batch of size 3.\n",
    "    #   Second is a batch of size 1\n",
    "    [\n",
    "        # First Batch is size 3\n",
    "        [\n",
    "            # 3 samples of features.\n",
    "            # There are 4 features per sample.\n",
    "            ['F11', 'F12', 'F13', 'F14'],\n",
    "            ['F21', 'F22', 'F23', 'F24'],\n",
    "            ['F31', 'F32', 'F33', 'F34']\n",
    "        ], [\n",
    "            # 3 samples of labels.\n",
    "            # There are 2 labels per sample.\n",
    "            ['L11', 'L12'],\n",
    "            ['L21', 'L22'],\n",
    "            ['L31', 'L32']\n",
    "        ]\n",
    "    ], [\n",
    "        # Second Batch is size 1.\n",
    "        # Since batch size is 3, there is only one sample left from the 4 samples.\n",
    "        [\n",
    "            # 1 sample of features.\n",
    "            ['F41', 'F42', 'F43', 'F44']\n",
    "        ], [\n",
    "            # 1 sample of labels.\n",
    "            ['L41', 'L42']\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    batches = list()\n",
    "    num_samples = len(features)\n",
    "    for start_i in range(0, num_samples, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        batches.append(batch)\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy: 0.11819999665021896\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    # for batch_features, batch_labels in ______\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epochs\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 11.2     Valid Accuracy: 0.0812\n",
      "Epoch: 1    - Cost: 10.5     Valid Accuracy: 0.093\n",
      "Epoch: 2    - Cost: 9.93     Valid Accuracy: 0.11 \n",
      "Epoch: 3    - Cost: 9.5      Valid Accuracy: 0.129\n",
      "Epoch: 4    - Cost: 9.11     Valid Accuracy: 0.143\n",
      "Epoch: 5    - Cost: 8.76     Valid Accuracy: 0.159\n",
      "Epoch: 6    - Cost: 8.44     Valid Accuracy: 0.174\n",
      "Epoch: 7    - Cost: 8.14     Valid Accuracy: 0.189\n",
      "Epoch: 8    - Cost: 7.87     Valid Accuracy: 0.205\n",
      "Epoch: 9    - Cost: 7.61     Valid Accuracy: 0.22 \n",
      "Epoch: 10   - Cost: 7.37     Valid Accuracy: 0.236\n",
      "Epoch: 11   - Cost: 7.15     Valid Accuracy: 0.249\n",
      "Epoch: 12   - Cost: 6.94     Valid Accuracy: 0.266\n",
      "Epoch: 13   - Cost: 6.74     Valid Accuracy: 0.281\n",
      "Epoch: 14   - Cost: 6.56     Valid Accuracy: 0.295\n",
      "Epoch: 15   - Cost: 6.38     Valid Accuracy: 0.311\n",
      "Epoch: 16   - Cost: 6.22     Valid Accuracy: 0.326\n",
      "Epoch: 17   - Cost: 6.06     Valid Accuracy: 0.337\n",
      "Epoch: 18   - Cost: 5.91     Valid Accuracy: 0.348\n",
      "Epoch: 19   - Cost: 5.77     Valid Accuracy: 0.36 \n",
      "Epoch: 20   - Cost: 5.63     Valid Accuracy: 0.371\n",
      "Epoch: 21   - Cost: 5.5      Valid Accuracy: 0.382\n",
      "Epoch: 22   - Cost: 5.38     Valid Accuracy: 0.396\n",
      "Epoch: 23   - Cost: 5.26     Valid Accuracy: 0.407\n",
      "Epoch: 24   - Cost: 5.14     Valid Accuracy: 0.416\n",
      "Epoch: 25   - Cost: 5.03     Valid Accuracy: 0.429\n",
      "Epoch: 26   - Cost: 4.93     Valid Accuracy: 0.441\n",
      "Epoch: 27   - Cost: 4.83     Valid Accuracy: 0.449\n",
      "Epoch: 28   - Cost: 4.73     Valid Accuracy: 0.457\n",
      "Epoch: 29   - Cost: 4.64     Valid Accuracy: 0.464\n",
      "Epoch: 30   - Cost: 4.55     Valid Accuracy: 0.47 \n",
      "Epoch: 31   - Cost: 4.46     Valid Accuracy: 0.479\n",
      "Epoch: 32   - Cost: 4.38     Valid Accuracy: 0.489\n",
      "Epoch: 33   - Cost: 4.3      Valid Accuracy: 0.499\n",
      "Epoch: 34   - Cost: 4.22     Valid Accuracy: 0.508\n",
      "Epoch: 35   - Cost: 4.14     Valid Accuracy: 0.516\n",
      "Epoch: 36   - Cost: 4.07     Valid Accuracy: 0.52 \n",
      "Epoch: 37   - Cost: 4.0      Valid Accuracy: 0.527\n",
      "Epoch: 38   - Cost: 3.93     Valid Accuracy: 0.536\n",
      "Epoch: 39   - Cost: 3.87     Valid Accuracy: 0.541\n",
      "Epoch: 40   - Cost: 3.8      Valid Accuracy: 0.546\n",
      "Epoch: 41   - Cost: 3.74     Valid Accuracy: 0.552\n",
      "Epoch: 42   - Cost: 3.68     Valid Accuracy: 0.558\n",
      "Epoch: 43   - Cost: 3.63     Valid Accuracy: 0.565\n",
      "Epoch: 44   - Cost: 3.57     Valid Accuracy: 0.571\n",
      "Epoch: 45   - Cost: 3.52     Valid Accuracy: 0.575\n",
      "Epoch: 46   - Cost: 3.47     Valid Accuracy: 0.579\n",
      "Epoch: 47   - Cost: 3.41     Valid Accuracy: 0.583\n",
      "Epoch: 48   - Cost: 3.37     Valid Accuracy: 0.587\n",
      "Epoch: 49   - Cost: 3.32     Valid Accuracy: 0.591\n",
      "Epoch: 50   - Cost: 3.27     Valid Accuracy: 0.596\n",
      "Epoch: 51   - Cost: 3.23     Valid Accuracy: 0.599\n",
      "Epoch: 52   - Cost: 3.19     Valid Accuracy: 0.604\n",
      "Epoch: 53   - Cost: 3.14     Valid Accuracy: 0.608\n",
      "Epoch: 54   - Cost: 3.1      Valid Accuracy: 0.612\n",
      "Epoch: 55   - Cost: 3.06     Valid Accuracy: 0.616\n",
      "Epoch: 56   - Cost: 3.03     Valid Accuracy: 0.619\n",
      "Epoch: 57   - Cost: 2.99     Valid Accuracy: 0.624\n",
      "Epoch: 58   - Cost: 2.95     Valid Accuracy: 0.628\n",
      "Epoch: 59   - Cost: 2.92     Valid Accuracy: 0.631\n",
      "Epoch: 60   - Cost: 2.88     Valid Accuracy: 0.635\n",
      "Epoch: 61   - Cost: 2.85     Valid Accuracy: 0.638\n",
      "Epoch: 62   - Cost: 2.82     Valid Accuracy: 0.641\n",
      "Epoch: 63   - Cost: 2.79     Valid Accuracy: 0.643\n",
      "Epoch: 64   - Cost: 2.76     Valid Accuracy: 0.647\n",
      "Epoch: 65   - Cost: 2.73     Valid Accuracy: 0.649\n",
      "Epoch: 66   - Cost: 2.7      Valid Accuracy: 0.652\n",
      "Epoch: 67   - Cost: 2.67     Valid Accuracy: 0.656\n",
      "Epoch: 68   - Cost: 2.64     Valid Accuracy: 0.659\n",
      "Epoch: 69   - Cost: 2.61     Valid Accuracy: 0.661\n",
      "Epoch: 70   - Cost: 2.59     Valid Accuracy: 0.663\n",
      "Epoch: 71   - Cost: 2.56     Valid Accuracy: 0.666\n",
      "Epoch: 72   - Cost: 2.54     Valid Accuracy: 0.669\n",
      "Epoch: 73   - Cost: 2.51     Valid Accuracy: 0.67 \n",
      "Epoch: 74   - Cost: 2.49     Valid Accuracy: 0.672\n",
      "Epoch: 75   - Cost: 2.47     Valid Accuracy: 0.674\n",
      "Epoch: 76   - Cost: 2.44     Valid Accuracy: 0.676\n",
      "Epoch: 77   - Cost: 2.42     Valid Accuracy: 0.678\n",
      "Epoch: 78   - Cost: 2.4      Valid Accuracy: 0.681\n",
      "Epoch: 79   - Cost: 2.38     Valid Accuracy: 0.683\n",
      "Epoch: 80   - Cost: 2.36     Valid Accuracy: 0.685\n",
      "Epoch: 81   - Cost: 2.34     Valid Accuracy: 0.687\n",
      "Epoch: 82   - Cost: 2.32     Valid Accuracy: 0.689\n",
      "Epoch: 83   - Cost: 2.3      Valid Accuracy: 0.692\n",
      "Epoch: 84   - Cost: 2.28     Valid Accuracy: 0.695\n",
      "Epoch: 85   - Cost: 2.26     Valid Accuracy: 0.696\n",
      "Epoch: 86   - Cost: 2.24     Valid Accuracy: 0.698\n",
      "Epoch: 87   - Cost: 2.22     Valid Accuracy: 0.699\n",
      "Epoch: 88   - Cost: 2.21     Valid Accuracy: 0.701\n",
      "Epoch: 89   - Cost: 2.19     Valid Accuracy: 0.703\n",
      "Epoch: 90   - Cost: 2.17     Valid Accuracy: 0.705\n",
      "Epoch: 91   - Cost: 2.16     Valid Accuracy: 0.708\n",
      "Epoch: 92   - Cost: 2.14     Valid Accuracy: 0.71 \n",
      "Epoch: 93   - Cost: 2.13     Valid Accuracy: 0.712\n",
      "Epoch: 94   - Cost: 2.11     Valid Accuracy: 0.713\n",
      "Epoch: 95   - Cost: 2.1      Valid Accuracy: 0.715\n",
      "Epoch: 96   - Cost: 2.08     Valid Accuracy: 0.716\n",
      "Epoch: 97   - Cost: 2.07     Valid Accuracy: 0.717\n",
      "Epoch: 98   - Cost: 2.05     Valid Accuracy: 0.718\n",
      "Epoch: 99   - Cost: 2.04     Valid Accuracy: 0.72 \n",
      "Test Accuracy: 0.708299994468689\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ReLU\n",
    "Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems.\n",
    "<img src=\"relu-network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.11000013   8.44000053]\n",
      " [  0.           0.        ]\n",
      " [ 24.01000214  38.23999786]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([\n",
    "    [1.0, 2.0, 3.0, 4.0],\n",
    "    [-1.0, -2.0, -3.0, -4.0],\n",
    "    [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer_output = tf.nn.relu(hidden_layer)\n",
    "output = tf.add(tf.matmul(hidden_layer_output, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    output = session.run(output)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropout\n",
    "Dropout is a regularization technique for reducing overfitting. The technique temporarily drops units (artificial neurons) from the network, along with all of those units' incoming and outgoing connections. Figure 1 illustrates how dropout works.\n",
    "<img src=\"dropout-node.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.57999945   8.45999908]\n",
      " [  0.           0.        ]\n",
      " [ 43.30000305  48.15999985]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "\n",
    "# TODO: Print logits from a session\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    logits = session.run(logits, feed_dict={keep_prob: 0.5})\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
